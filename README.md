Master thesis - QuOTA: Improving LLM agents with a queue
========================================================

Overview
--------

### Data

Inside the *data* folder you will find the data needed to run our experiments. Moreover, you will find the following notebooks:

- [data_collector.ipynb](./data/data_collector.ipynb). Reformats the datasets mentioned below into a common format, such that it can be run in the experiments.
- [data_insight.ipynb](./data/data_insight.ipynb). A notebook to gain insights on the data, including visualizations and statistics.

#### Datasets

This project contains the following datasets.

Note that *Reverse Chain* has not been used for the experiments, since *Reverse Chain* is too advanced for the current setup. More specifically, it requires a more sophisticated approach for the Queue system as it contains chained function calls.

1. MetaTool (ToolE)
   - [arxiv (paper)](https://arxiv.org/abs/2310.03128)
   - [Github](https://github.com/HowieHwong/MetaTool?tab=readme-ov-file)
2. ToolLens
   - [arxiv (paper)](https://arxiv.org/abs/2405.16089)
   - [Papers with code](https://paperswithcode.com/dataset/toollens)
   - [Hugging Face](https://huggingface.co/datasets/stuedu/ToolLens)
3. Berkeley-Function-Calling-Leaderboard
   - [Official site](https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html)
   - [Hugging Face](https://huggingface.co/datasets/gorilla-llm/Berkeley-Function-Calling-Leaderboard)
4. Reverse Chain
   - [arxiv (paper)](https://arxiv.org/abs/2310.04474)
   - [Github](https://github.com/zhangyingerjelly/reverse-chain)

### Experiments

Inside the *experiments* folder you will find the following 

- `run_experiment.py`. Python script to run the experiments. See Quick start for more details.
- [test_run.ipynb](./experiments/test_run.ipynb). A notebook to run a single sample, and inspect the process.
- [semantic_cutoff.ipynb](./experiments/semantic_cutoff.ipynb). A notebook to analyze the semantic cutoff.
- [inspect.ipynb](./experiments/inspect.ipynb). A notebook to inspect the experiment results generated by *run_experiment.py*.

Quick start
-----------

To get started with the experiments, follow these steps:

The framework supports OpenAI (not GPT-5 models), TogetherAI and Groq models.

See the official documentation for more details:
- [OpenAI](https://platform.openai.com/docs/quickstart)
- [TogetherAI](https://docs.together.ai/docs/quickstart)
- [Groq](https://console.groq.com/docs/quickstart)

Now create a python environment `.env` in the root directory of the project and activate it, or use the global python environment if you prefer.

Install the required dependencies:

```bash
pip install -r requirements.txt
```
Must enter *experiments* folder in order to run the experiment
```bash
cd experiments
```
To run the experiment:
```bash
python run_experiment.py \
  --client_type <CLIENT_ENUM_NAME> \
  --model_name <MODEL_ID> \
  --source <DATASET_NAME> \
  --experiment_type <EXPERIMENT_ENUM_NAME> \
  [--max_num_samples <INT>]
```

**Parameters**

* `--client_type`: Choose between `OpenAI`, `Together`, `Groq`.
* `--model_name`: model name such as `'gpt-4o-mini'`.
* `--source` â€” dataset/source name. Choose between `ToolE`, `Berkeley`, `ToolLens`, `ReverseChain`.
* `--experiment_type`: Choose between `REACT`, `REACT_SINGLE_STEP`, `REACT_WITH_CO_OCCURRENCE`, `QUOTA`, `QUOTA_WITH_REFLECTION`.
* `--max_num_samples` **(Optional)**: positive integer to cap how many samples to run. Run the full set if omitted.

If a required flag is missing or invalid, the script will raise a clear error indicating what to fix.

After the run completes, inspect results with the [inspect.ipynb](./experiments/inspect.ipynb) notebook.
